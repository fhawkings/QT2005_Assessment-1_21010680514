{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "e2475ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/felixhawkings/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/felixhawkings/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixhawkings/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/felixhawkings/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixhawkings/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/felixhawkings/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PRAW in ./opt/anaconda3/lib/python3.9/site-packages (7.6.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in ./opt/anaconda3/lib/python3.9/site-packages (from PRAW) (2.3.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in ./opt/anaconda3/lib/python3.9/site-packages (from PRAW) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in ./opt/anaconda3/lib/python3.9/site-packages (from PRAW) (1.5.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in ./opt/anaconda3/lib/python3.9/site-packages (from prawcore<3,>=2.1->PRAW) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->PRAW) (2.0.4)\n",
      "Requirement already satisfied: vaderSentiment in ./opt/anaconda3/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.9/site-packages (from vaderSentiment) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: wordcloud in ./opt/anaconda3/lib/python3.9/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in ./opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: matplotlib in ./opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pillow in ./opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: six in ./opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "import networkx as nx\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.metrics import jaccard_distance\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "nltk.download('vader_lexicon')\n",
    "punct = list(string.punctuation)\n",
    "!pip install PRAW\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import praw\n",
    "import datetime\n",
    "import nltk.sentiment.vader as vd\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import IFrame\n",
    "import random\n",
    "from collections import Counter\n",
    "import random\n",
    "from random import sample\n",
    "from scipy.stats import entropy\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader_lexicon = SentimentIntensityAnalyzer()\n",
    "from wordcloud import WordCloud\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "36354300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing the bandwidth of the jupiter notebook to allow for the qaunitities of data\n",
    "%config NotebookApp.iopub_data_rate_limit = 10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb1e46",
   "metadata": {},
   "source": [
    "FICTION TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing the search parameters for the Gutendex API\n",
    "params_g = {'topic':'fiction'}\n",
    "\n",
    "# Define the base URL for the Gutendex API\n",
    "gut = 'https://gutendex.com/books/'\n",
    "\n",
    "# Send a GET request to the Gutendex API with the search parameters\n",
    "rg = requests.get(url = gut, params = params_g)\n",
    "\n",
    "# Get the response from the API in JSON format\n",
    "rg = rg.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the desktop directory where the text files will be saved\n",
    "desktop_path = '/filepathname/'\n",
    "\n",
    "# Define a list of URLs for the text files to be downloaded\n",
    "urls = [\n",
    "    'https://www.gutenberg.org/ebooks/84.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/36.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/42324.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/41445.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/21279.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/164.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/5230.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/159.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/32032.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/61309.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/86.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/69762.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/139.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/201.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/18857.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/69703.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/19141.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/61963.txt.utf-8',\n",
    "    \n",
    "]\n",
    "\n",
    "# Loop through each URL in the list of URLs\n",
    "for url in urls:\n",
    "    # Send a GET request to the URL to download the text file\n",
    "    response = requests.get(url)\n",
    "    # Get the filename from the URL and use it to create the name of the local file\n",
    "    filename = f\"{url.split('/')[-1].split('.')[0]}.txt\"\n",
    "    # Combine the desktop path with the filename to create the full path to the local file\n",
    "    filepath = os.path.join(desktop_path, filename)\n",
    "    # Write the contents of the downloaded text file to the local file\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "3636d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to hold the data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through each text file and read it into a pandas dataframe\n",
    "for file in glob.glob(\"/filepathname/*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        # create a new dataframe with the file contents and add it to the main dataframe\n",
    "        df_new = pd.DataFrame({'text': [content]})\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "20e7195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying ascii to entire dataframe\n",
    "df = df.applymap(lambda x: x.encode('ascii', 'ignore').decode('ascii') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "b72cee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function that takes a sentence and returns its lemmatized version\n",
    "def lemmatize1_sentence(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokenized_words = word_tokenize(sentence)\n",
    "    # Lemmatize each word in the sentence and convert it to lowercase\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word.lower()) for word in tokenized_words]\n",
    "    # Join the lemmatized words back into a sentence and return it\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "# Apply the lemmatize1_sentence function to the 'text' column of the DataFrame 'df',\n",
    "# Store the lemmatized output in a new column called 'lemmatized_text_lower'\n",
    "df['lemmatized_text_lower'] = df['text'].apply(lemmatize1_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "4bf47c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define a function to tokenize a sentence\n",
    "def tokenize1_sentence(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "# apply the function to the 'lemmatized_text_lower' column and save the result to a new column 'tokenized1_words'\n",
    "df['tokenized1_words'] = df['lemmatized_text_lower'].apply(tokenize1_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "6984c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/felixhawkings/Desktop/csv/my_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "ed5110ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Randomly selecting 20% of the rows from the data frame\n",
    "sample_df = df.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# Dropping the extra column from the sample data frame\n",
    "sample_df = sample_df.drop(columns=['lemmatized_text_lower'])\n",
    "\n",
    "# Applying VAD analysis on the sample data frame\n",
    "vad_df = sample_df['text'].apply(lambda x: analyzer.polarity_scores(x)).apply(pd.Series)\n",
    "\n",
    "# Renaming columns\n",
    "vad_df.columns = ['Valence', 'Arousal', 'Dominance']\n",
    "\n",
    "# Combining the VAD data frame with the sample data frame\n",
    "sample_df = pd.concat([sample_df, vad_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "9d29bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the VAD values for each word\n",
    "def calculate_vad_scores(text):\n",
    "    scores = vader_lexicon.polarity_scores(text)\n",
    "    valence = scores['pos'] - scores['neg']\n",
    "    arousal = scores['pos'] + scores['neg'] - scores['neu']\n",
    "    dominance = scores['pos'] + scores['neg'] + scores['neu']\n",
    "    return valence, arousal, dominance\n",
    "\n",
    "# Take a random sample of 10% of the data\n",
    "random_sample = df.sample(frac=0.9, random_state=42)\n",
    "\n",
    "# Apply the function to the tokenized words in the random sample\n",
    "random_sample[['Valence', 'Arousal', 'Dominance']] = random_sample['tokenized1_words'].apply(lambda x: pd.Series(calculate_vad_scores(x)))\n",
    "\n",
    "# Combine the random sample back into the original dataframe\n",
    "df.update(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "86e7909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the random sample back into the original dataframe\n",
    "df[['Valence', 'Arousal', 'Dominance']] = random_sample[['Valence', 'Arousal', 'Dominance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "611f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['string1_words'] = df['tokenized1_words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "122830ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the words in the 'text' column and store them in a list\n",
    "tokenized_words = [nltk.word_tokenize(text) for text in df['text']]\n",
    "\n",
    "# concatenate the tokenized words into a single list\n",
    "string1_words = [word for words in tokenized_words for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923add5",
   "metadata": {},
   "source": [
    "SCIENCE FICTION TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing the search parameters for the Gutendex API\n",
    "params_g = {'topic':'science fiction'}\n",
    "\n",
    "# Define the base URL for the Gutendex API\n",
    "gut = 'https://gutendex.com/books/'\n",
    "\n",
    "# Send a GET request to the Gutendex API with the search parameters\n",
    "rg = requests.get(url = gut, params = params_g)\n",
    "\n",
    "# Get the response from the API in JSON format\n",
    "rg = rg.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab008ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the desktop directory where the text files will be saved\n",
    "desktop_path = '/filepathname/'\n",
    "\n",
    "# Define a list of URLs for the text files to be downloaded\n",
    "urls = ['https://www.gutenberg.org/ebooks/2641.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/145.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/37106.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/16389.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/67979.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/6761.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/394.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/1259.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/84.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/1342.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/11.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/174.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/64317.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/46.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/345.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/25344.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/20228.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/28054.txt.utf-8'\n",
    "]\n",
    "\n",
    "# Loop through each URL in the list of URLs\n",
    "for url in urls:\n",
    "    # Send a GET request to the URL to download the text file\n",
    "    response = requests.get(url)\n",
    "    # Get the filename from the URL and use it to create the name of the local file\n",
    "    filename = f\"{url.split('/')[-1].split('.')[0]}.txt\"\n",
    "    # Combine the desktop path with the filename to create the full path to the local file\n",
    "    filepath = os.path.join(desktop_path, filename)\n",
    "    # Write the contents of the downloaded text file to the local file\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "6eacf739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to hold the data\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "# loop through each text file and read it into a pandas dataframe\n",
    "for file in glob.glob(\"/filepathname/*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        # create a new dataframe with the file contents and add it to the main dataframe\n",
    "        df2_new = pd.DataFrame({'text': [content]})\n",
    "        df2 = pd.concat([df2, df2_new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "82064e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying ascii to entire dataframe\n",
    "df2 = df2.applymap(lambda x: x.encode('ascii', 'ignore').decode('ascii') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "40a081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns={'text': 'text2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "407022b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying ascii to entire dataframe\n",
    "df2 = df2.applymap(lambda x: x.encode('ascii', 'ignore').decode('ascii') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "8d331b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function that takes a sentence and returns its lemmatized version\n",
    "def lemmatize2_sentence(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokenized2_words = word_tokenize(sentence)\n",
    "    # Lemmatize each word in the sentence and convert it to lowercase\n",
    "    lemmatized2_text = [lemmatizer.lemmatize(word.lower()) for word in tokenized2_words]\n",
    "    # Join the lemmatized words back into a sentence and return it\n",
    "    return \" \".join(lemmatized2_text)\n",
    "\n",
    "# Apply the lemmatize1_sentence function to the 'text' column of the DataFrame 'df',\n",
    "# and store the lemmatized output in a new column called 'lemmatized_text_lower'\n",
    "df2['lemmatized_text_lower2'] = df2['text2'].apply(lemmatize2_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "128810ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define a function to tokenize a sentence\n",
    "def tokenize2_sentence(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "# apply the function to the 'lemmatized_text_lower' column and save the result to a new column 'tokenized1_words'\n",
    "df2['tokenized2_words'] = df2['lemmatized_text_lower2'].apply(tokenize2_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "a855b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of SentimentIntensityAnalyzer\n",
    "analyzer2 = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Randomly selecting 20% of the rows from the data frame\n",
    "sample2_df = df.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# Dropping the extra column from the sample data frame\n",
    "sample2_df = sample2_df.drop(columns=['lemmatized_text_lower'])\n",
    "\n",
    "# Applying VAD analysis on the sample data frame\n",
    "vad2_df = sample2_df['text'].apply(lambda x: analyzer.polarity_scores(x)).apply(pd.Series)\n",
    "\n",
    "# Renaming columns\n",
    "vad2_df.columns = ['Valence', 'Arousal', 'Dominance']\n",
    "\n",
    "# Combining the VAD data frame with the sample data frame\n",
    "sample2_df = pd.concat([sample2_df, vad2_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "e3c5c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VADER lexicon\n",
    "vader_lexicon = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to calculate the VAD values for each word\n",
    "def calculate_vad_scores(text):\n",
    "    scores = vader_lexicon.polarity_scores(text)\n",
    "    valence = scores['pos'] - scores['neg']\n",
    "    arousal = scores['pos'] + scores['neg'] - scores['neu']\n",
    "    dominance = scores['pos'] + scores['neg'] + scores['neu']\n",
    "    return valence, arousal, dominance\n",
    "\n",
    "# Take a random sample of 10% of the data\n",
    "random2_sample = df2.sample(frac=0.9, random_state=42)\n",
    "\n",
    "# Apply the function to the tokenized words in the random sample\n",
    "random2_sample[['Valence', 'Arousal', 'Dominance']] = random2_sample['tokenized2_words'].apply(lambda x: pd.Series(calculate_vad_scores(x)))\n",
    "\n",
    "# Combine the random sample back into the original dataframe\n",
    "df2.update(random2_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "9575011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the random sample back into the original dataframe\n",
    "df2[['Valence', 'Arousal', 'Dominance']] = random2_sample[['Valence', 'Arousal', 'Dominance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "40b4ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['string2_words'] = df2['tokenized2_words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "a8cc25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the words in the 'text' column and store them in a list\n",
    "tokenized2_words = [nltk.word_tokenize(text) for text in df['text']]\n",
    "\n",
    "# concatenate the tokenized words into a single list\n",
    "string2_words = [word for words in tokenized2_words for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "556ce0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('QT2005_Assessment#1_21010680514_Fiction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "70a89b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('QT2005_Assessment#1_21010680514_Science_Fiction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
